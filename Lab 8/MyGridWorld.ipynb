{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment :\n",
    "    def __init__(self, R, P_actionSuccess, gamma) :\n",
    "        self.R = R   # m*n reward matrix\n",
    "        self.P_actionSuccess = P_actionSuccess\n",
    "        self.m = R.shape[0]\n",
    "        self.n = R.shape[1]\n",
    "        self.gamma = gamma\n",
    "        self.V = np.zeros(R.shape)\n",
    "        self.policy = np.array([[\"right\", \"right\", \"right\", \"right\"]\n",
    "                        ,[\"right\", \"right\", \"right\", \"right\"]\n",
    "                        ,[\"right\", \"right\", \"right\", \"right\"]\n",
    "                        ,[\"right\", \"right\", \"right\", \"right\"]])\n",
    "        self.actionList = [\"left\", \"right\", \"up\", \"down\"]\n",
    "        \n",
    "    def getSuccessor(self, i, j, action) :\n",
    "        if(action == \"left\" and j > 0) :\n",
    "            return self.V[i][j-1]\n",
    "        elif(action == \"right\" and j < self.n-1)  :\n",
    "            return self.V[i][j+1]\n",
    "        elif(action == \"up\" and i > 0) :\n",
    "            return self.V[i-1][j]\n",
    "        elif(action == \"down\" and i < self.m-1) :\n",
    "            return self.V[i+1][j]\n",
    "        return None\n",
    "        \n",
    "    def Bellmann(self, i, j) :\n",
    "        bestAction = \"None\"\n",
    "        max_val = -sys.maxsize\n",
    "        reward = self.R[i][j]\n",
    "        for action in self.actionList :\n",
    "            val = 0\n",
    "            for k, p in enumerate(self.P_actionSuccess[action]) :\n",
    "                V_successor = self.getSuccessor(i, j, self.actionList[k])\n",
    "                if(V_successor is not None) :\n",
    "                    val += (p * (reward + (gamma * V_successor)))\n",
    "                else :\n",
    "                    val += p*-1  # reward = -1 \n",
    "            if(val > max_val) :\n",
    "                max_val = val\n",
    "                bestAction = action\n",
    "        return max_val, bestAction\n",
    "        \n",
    "    def ValueIteration(self) :\n",
    "        itr = 0\n",
    "        while(itr < 1000) :\n",
    "            itr += 1\n",
    "            delta = 0\n",
    "            for i in range(self.m) :\n",
    "                for j in range(self.n) :\n",
    "                    v = self.V[i][j]\n",
    "                    self.V[i][j], self.policy[i][j] = self.Bellmann(i, j)\n",
    "                    delta = max(delta, abs(v - self.V[i][j]))\n",
    "            if(delta < 0.01) :\n",
    "                print(\"Values converged....\")\n",
    "                break\n",
    "        print(\"(itr, delta) = \", (itr, delta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Driver Program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________Value Iteration_________________\n",
      "\n",
      "Rewards\n",
      " [[0.   0.45 1.   0.9 ]\n",
      " [0.23 1.25 0.   0.  ]\n",
      " [0.   0.45 0.75 0.  ]\n",
      " [0.85 1.5  2.5  0.85]]\n",
      "Values converged....\n",
      "(itr, delta) =  (116, 0.009599949467087754)\n",
      "Optimal Value\n",
      " [[46.03452537 52.34481561 52.49861186 46.92055101]\n",
      " [52.08833756 53.88631892 53.28901991 51.55621296]\n",
      " [52.12638615 53.97629378 54.80036568 52.85971308]\n",
      " [48.08469127 53.97737426 55.59698689 49.4332896 ]]\n",
      "Optimal Policy\n",
      " [['right' 'down' 'down' 'left']\n",
      " ['right' 'down' 'down' 'left']\n",
      " ['right' 'right' 'down' 'left']\n",
      " ['right' 'up' 'up' 'left']]\n"
     ]
    }
   ],
   "source": [
    "P_actionSuccess = { \"left\" : [0.8, 0, 0.1, 0.1], \"right\" : [0, 0.8, 0.1, 0.1], \n",
    "                                 \"up\" : [0.1, 0.1, 0.8, 0], \"down\" : [0.1, 0.1, 0, 0.8] }\n",
    "R = np.array([[0, 0.45, 1, 0.9]\n",
    "               ,[0.23, 1.25, 0, 0]\n",
    "               ,[0, 0.45, 0.75, 0]\n",
    "               ,[0.85, 1.5, 2.5, 0.85]])\n",
    "# R = np.array([[0.4, 0.1, 0.5, 0.8],\n",
    "#               [0.2, 0.6, 0.19, 0.3],\n",
    "#               [0.11, 0.26, 0.6, 0.7],\n",
    "#               [0.10, 0.2, 0.3, 0.4]])\n",
    "                   \n",
    "# R = np.array([[4, 1, 5, 8],\n",
    "#               [2, 6, 19, 3],\n",
    "#               [11, 26, 6, 7],\n",
    "#               [10, 2, 3, 4]])\n",
    "gamma = 0.98\n",
    "envObj = Environment(R, P_actionSuccess, gamma)\n",
    "print(\"_________________Value Iteration_________________\\n\")\n",
    "print(\"Rewards\\n\", R)\n",
    "envObj.ValueIteration()\n",
    "print(\"Optimal Value\\n\", envObj.V)\n",
    "print(\"Optimal Policy\\n\", envObj.policy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
