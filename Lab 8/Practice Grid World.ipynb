{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid World"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment :\n",
    "    def __init__(self, R, P_actionSuccess, gamma) :\n",
    "        self.R = R\n",
    "        self.m = R.shape[0]\n",
    "        self.n = R.shape[1]\n",
    "        self.gamma = gamma\n",
    "        self.P_actionSuccess = P_actionSuccess\n",
    "        \n",
    "        # initialize value and policy\n",
    "        self.V = np.zeros(R.shape)\n",
    "        self.policy = np.array(np.empty(R.shape), dtype=np.str)\n",
    "#         self.V = np.zeros(R.shape)\n",
    "#         self.policy = np.array([[\"right\", \"right\", \"right\", \"right\"]\n",
    "#                         ,[\"right\", \"right\", \"right\", \"right\"]\n",
    "#                         ,[\"right\", \"right\", \"right\", \"right\"]\n",
    "#                         ,[\"right\", \"right\", \"right\", \"right\"]])\n",
    "        self.policy[:] = \"left\"\n",
    "        self.actionList = [\"left\", \"right\", \"up\", \"down\"]\n",
    "\n",
    "    def getSuccessor(self, i, j, action) :\n",
    "        if(action == \"left\" and j > 0) :\n",
    "            return self.V[i][j-1]\n",
    "        elif(action == \"right\" and j < self.n-1)  :\n",
    "            return self.V[i][j+1]\n",
    "        elif(action == \"up\" and i > 0) :\n",
    "            return self.V[i-1][j]\n",
    "        elif(action == \"down\" and i < self.m-1) :\n",
    "            return self.V[i+1][j]\n",
    "        return None\n",
    "  \n",
    "    def Bellmann(self, i , j) :\n",
    "        bestAction = \"None\"\n",
    "        bestValue = -sys.maxsize\n",
    "        reward = self.R[i, j]\n",
    "        for action in self.actionList :\n",
    "            val = 0\n",
    "            for k, p in enumerate(self.P_actionSuccess[action]) :\n",
    "                V_successor = self.getSuccessor(i, j, self.actionList[k])\n",
    "                if(V_successor is not None) :\n",
    "                    val += (p * (reward + (gamma*V_successor)))\n",
    "#                     val += p*-1\n",
    "                else :\n",
    "                    V_successor = self.V[i, j]\n",
    "                    val += p * (-1 + (gamma*V_successor))\n",
    "                \n",
    "            if(val > bestValue) :\n",
    "                bestValue = val\n",
    "                bestAction = action\n",
    "\n",
    "        return bestValue, bestAction\n",
    "        \n",
    "    def ValueIteration(self) :\n",
    "        iter = 0\n",
    "        MAX_ITER = 10000\n",
    "        print(50*'-')\n",
    "        print(' '*15, \"Value Iteration \")\n",
    "        print(50*'-')\n",
    "        while(iter < MAX_ITER) :\n",
    "            iter += 1\n",
    "            delta = 0\n",
    "            for i in range(self.m) :\n",
    "                for j in range(self.n) :\n",
    "                    oldV = self.V[i, j]\n",
    "                    self.V[i, j], self.policy[i, j] = self.Bellmann(i, j)\n",
    "                    delta = max(delta, abs(oldV - self.V[i, j]))\n",
    "#             print(self.V)\n",
    "#             print(\"delta = \", delta)\n",
    "            if(delta < 0.01) :\n",
    "                break\n",
    "                \n",
    "        if(iter == MAX_ITER) :\n",
    "            print(\"\\nValue iteration did not converge in\", iter, \"iterations\")\n",
    "        else :\n",
    "            print(\"\\nValue Iteration converges in\", iter, \"steps\")\n",
    "            print(\"\\nOptimal Value : \\n\", self.V)\n",
    "            print(\"\\nOptimal Policy : \\n\", self.policy)\n",
    "        print(50*'-')\n",
    "        \n",
    "    def findNewPolicy(self) :\n",
    "        iter = 0\n",
    "        MAX_ITER = 10000\n",
    "        # Policy Evaluation\n",
    "        while(iter < MAX_ITER) :\n",
    "            iter += 1\n",
    "            delta = 0\n",
    "            for i in range(self.m) :\n",
    "                for j in range(self.n) :\n",
    "                    prevVal = self.V[i, j]\n",
    "                    reward = self.R[i, j]\n",
    "                    action = self.policy[i, j]\n",
    "                    val = 0\n",
    "                    for k, p in enumerate(self.P_actionSuccess[action]) :\n",
    "                        V_successor = self.getSuccessor(i, j, self.actionList[k])\n",
    "                        if(V_successor is not None) :\n",
    "                            val += (p * (reward + (gamma*V_successor)))\n",
    "                        else :\n",
    "                            V_successor = self.V[i, j]\n",
    "                            val += (p * (-1 + (gamma * V_successor)))\n",
    "                    delta = max(delta, abs(val-prevVal))\n",
    "                    self.V[i, j] = val\n",
    "#             print(\"Value : \", self.V)\n",
    "\n",
    "            if(delta < 0.01) :\n",
    "                break\n",
    "#         print(\"Value : \", self.V)\n",
    "        # Policy Improvement\n",
    "        for i in range(self.m) :\n",
    "            for j in range(self.n) :\n",
    "                reward = self.R[i, j]\n",
    "                max_val = -1e7\n",
    "                bestAction = self.policy[i, j]\n",
    "                for action in self.actionList :\n",
    "                    val = 0\n",
    "                    for k, p in enumerate(self.P_actionSuccess[action]) :\n",
    "                        V_successor = self.getSuccessor(i, j, self.actionList[k])\n",
    "                        if(V_successor is not None) :\n",
    "                            val += (p * (reward + gamma * V_successor))\n",
    "                        else :\n",
    "                            V_successor = self.V[i, j]\n",
    "                            val += (p * (-1 + gamma * V_successor))\n",
    "                    if(max_val < val) :\n",
    "                        bestAction = action\n",
    "                        max_val = val\n",
    "                self.policy[i, j] = bestAction\n",
    "                    \n",
    "    def policyIteration(self) :\n",
    "        iter = 0\n",
    "        MAX_ITER = 10000\n",
    "        while(iter < MAX_ITER) :\n",
    "            iter += 1\n",
    "            prevPolicy = self.policy.copy()\n",
    "            self.findNewPolicy()\n",
    "            if(np.array_equal(prevPolicy, self.policy)) :\n",
    "                break\n",
    "                \n",
    "        if(iter == MAX_ITER) :\n",
    "            print(\"\\nPolicy iteration did not converge in\", iter, \"iterations\")\n",
    "        else :\n",
    "            print(\"\\nPolicy Iteration converges in\", iter, \"steps\")\n",
    "            print(\"\\nOptimal Value : \\n\", self.V)\n",
    "            print(\"\\nOptimal Policy : \\n\", self.policy)\n",
    "        print(50*'-')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Driver Program "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------- REWARD  ---------------\n",
      "[[0.   0.45 1.   0.9 ]\n",
      " [0.23 1.25 0.   0.  ]\n",
      " [0.   0.45 0.75 0.  ]\n",
      " [0.85 1.5  2.5  0.85]]\n",
      "---------------------------------------\n",
      "--------------------------------------------------\n",
      "                Value Iteration \n",
      "--------------------------------------------------\n",
      "\n",
      "Value Iteration converges in 156 steps\n",
      "\n",
      "Optimal Value : \n",
      " [[72.12506715 73.90485711 74.02050932 73.07299498]\n",
      " [73.72214822 75.4347782  74.77654057 73.17912364]\n",
      " [74.40108563 76.07149012 76.81170091 74.98386199]\n",
      " [76.22940681 77.56459677 78.14558431 76.80565289]]\n",
      "\n",
      "Optimal Policy : \n",
      " [['right' 'down' 'down' 'left']\n",
      " ['right' 'down' 'down' 'down']\n",
      " ['down' 'down' 'down' 'down']\n",
      " ['right' 'right' 'left' 'left']]\n",
      "--------------------------------------------------\n",
      "\n",
      "Policy Iteration converges in 5 steps\n",
      "\n",
      "Optimal Value : \n",
      " [[72.2175664  73.99610433 74.11015899 73.16102085]\n",
      " [73.8134898  75.52464177 74.86480626 73.26607925]\n",
      " [74.49134173 76.15990866 76.89849706 75.06924035]\n",
      " [76.31800903 77.65155342 78.2309454  76.88946675]]\n",
      "\n",
      "Optimal Policy : \n",
      " [['right' 'down' 'down' 'left']\n",
      " ['right' 'down' 'down' 'down']\n",
      " ['down' 'down' 'down' 'down']\n",
      " ['right' 'right' 'left' 'left']]\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "P_actionSuccess = { \"left\" : [0.8, 0, 0.1, 0.1], \"right\" : [0, 0.8, 0.1, 0.1], \n",
    "                                 \"up\" : [0.1, 0.1, 0.8, 0], \"down\" : [0.1, 0.1, 0, 0.8] }\n",
    "R = np.array([[0, 0.45, 1, 0.9]\n",
    "               ,[0.23, 1.25, 0, 0]\n",
    "               ,[0, 0.45, 0.75, 0]\n",
    "               ,[0.85, 1.5, 2.5, 0.85]])\n",
    "gamma = 0.98\n",
    "envObj1 = Environment(R, P_actionSuccess, gamma)\n",
    "envObj2 = Environment(R, P_actionSuccess, gamma)\n",
    "print(\"-\"*15, \"REWARD \", \"-\"*15)\n",
    "print(R)\n",
    "print(\"-\"*39)\n",
    "envObj1.ValueIteration()\n",
    "envObj2.policyIteration()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
