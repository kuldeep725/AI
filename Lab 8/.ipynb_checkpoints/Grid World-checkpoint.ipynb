{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab 08\n",
    "#### Problem: Grid World\n",
    "##### Members:\n",
    "        1. Amit Vikram Singh(111601001)\n",
    "        2. Kuldeep Singh Bhandari(111601009)\n",
    "Logic :\n",
    "\n",
    "> 1. **Value Iteration** : In value iteration, first we find optimal value for the given grid world and then find the optimal policy. \n",
    "2. **Policy Iteration** : In policy iteration, we have an initial pseudo policy and using that, we will find value corresponding to that policy and then we will update the policy in greedy fashion. We keep on doing that until current policy becomes equal to the previous policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tqdm'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-56158af02519>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mState\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tqdm'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "\n",
    "class State:\n",
    "    def __init__(self, i, j):\n",
    "        self.row = i\n",
    "        self.col = j\n",
    "\n",
    "    \n",
    "class Environment:\n",
    "    def __init__(self, n, m):\n",
    "        self.rowNum = n\n",
    "        self.colNum = m\n",
    "        \n",
    "        #reward for R: S -> relaNumber\n",
    "        self.reward = np.array([[0, 0.45, 1, 0.9]\n",
    "                               ,[0.23, 1.25, 0, 0]\n",
    "                               ,[0, 0.45, 0.75, 0]\n",
    "                               ,[0.85, 1.5, 2.5, 0.85]])\n",
    "        \n",
    "        #Initial Policy for policy iteration  \n",
    "        self.policy =  np.array([[\"right\", \"right\", \"right\", \"right\"]\n",
    "                                ,[\"right\", \"right\", \"right\", \"right\"]\n",
    "                                ,[\"right\", \"right\", \"right\", \"right\"]\n",
    "                                ,[\"right\", \"right\", \"right\", \"right\"]])\n",
    "        \n",
    "        #Probabilty P(action) : A x A-> realNumber\n",
    "                                    #left, right, up, down\n",
    "        self.probability = np.array([[0.8, 0, 0.1, 0.1]    #left\n",
    "                                    ,[0, 0.8, 0.1, 0.1]   #right\n",
    "                                    ,[0.1, 0.1 , 0.8, 0]  #up\n",
    "                                    ,[0.1, 0.1, 0, 0.8]])  #down\n",
    "        \n",
    "        #Initial Value Function\n",
    "        self.value = np.zeros((self.rowNum, self.colNum))\n",
    "        \n",
    "        #Possible Actions\n",
    "        self.action = {\"left\": (0, -1), \"right\": (0, 1), \"up\": (-1, 0), \"down\": (1, 0)}\n",
    "        \n",
    "    #fucntion which takes a state and checks if state if valid i.e state is not outside of grid\n",
    "    def isValidState(self, state):\n",
    "        return (state.row >=0 and state.row < self.rowNum and state.col < self.colNum and state.col >=0)\n",
    "        \n",
    "        \n",
    "    #function which takes current state and action AND returns nextState\n",
    "    def getNextState(self, state, action):\n",
    "        nextState = copy.copy(state)\n",
    "        nextState.row+=self.action[action][0]\n",
    "        nextState.col+=self.action[action][1]\n",
    "        \n",
    "        if(self.isValidState(nextState)):\n",
    "            return nextState\n",
    "        \n",
    "        return None\n",
    "        \n",
    "        \n",
    "class agent:\n",
    "    def __init__(self, row, col):\n",
    "        self.row = row\n",
    "        self.col = col"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Value Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexToAction = {0:\"left\", 1:\"right\", 2:\"up\", 3:\"down\"}\n",
    "actionToIndex = {\"left\":0, \"right\":1, \"up\":2, \"down\":3}\n",
    "\n",
    "env = Environment(4, 4)\n",
    "\n",
    "def normInfinity(currValue, optimalValue):\n",
    "    maxDiff = 0\n",
    "    for i in range(env.rowNum):\n",
    "        for j in range(env.colNum):\n",
    "            maxDiff = max(maxDiff, abs(currValue[i, j] - optimalValue[i, j]))\n",
    "    return maxDiff\n",
    "            \n",
    "def valueIteration(gamma):\n",
    "    print(\"_________________Value Iteration_________________\")\n",
    "    iter1 = 0\n",
    "    theta = 0.01\n",
    "    diff = []\n",
    "    while(True):\n",
    "        if(iter1>1000):\n",
    "            break\n",
    "        iter1+=1\n",
    "\n",
    "        delta = 0\n",
    "        for row in range(env.rowNum):\n",
    "            for col in range(env.colNum):\n",
    "                currState = State(row, col)\n",
    "                value = -100\n",
    "                prevValue = env.value[row, col]\n",
    "                for action in env.action:\n",
    "                    currReward = 0\n",
    "                    for i, p in enumerate(env.probability[actionToIndex[action]]):\n",
    "                        succAction = indexToAction[i]\n",
    "                        nextState = env.getNextState(currState, succAction)\n",
    "                        if(nextState is None):\n",
    "                            currReward+=(p*-1)\n",
    "                        else:\n",
    "                            currReward+=(p*(env.reward[currState.row, currState.col]\n",
    "                                            + gamma* env.value[nextState.row, nextState.col]))\n",
    "\n",
    "    #                 print(currState.row, currState.col, currReward)\n",
    "                    value = max(value, currReward)\n",
    "\n",
    "                env.value[row, col] = value\n",
    "                print(env.value)\n",
    "                delta = max(delta, abs(prevValue - env.value[row, col]))\n",
    "    #     print(delta)\n",
    "        if(delta < theta):\n",
    "            print(\"Values converged....\")\n",
    "            break\n",
    "            \n",
    "    #get Optimal Policy\n",
    "    for row in range(env.rowNum):\n",
    "        for col in range(env.colNum):\n",
    "            currState = State(row, col)\n",
    "            maxReward = -10\n",
    "            optimalAction = \"left\"\n",
    "            for action in env.action:\n",
    "                currReward = 0\n",
    "                for i, p in enumerate(env.probability[actionToIndex[action]]):\n",
    "                    succAction = indexToAction[i]\n",
    "                    nextState = env.getNextState(currState, succAction)\n",
    "                    if(nextState is None):\n",
    "                        currReward+=(p*-1)\n",
    "                    else:\n",
    "                        currReward+=(p*(env.reward[currState.row, currState.col]\n",
    "                                        + gamma* env.value[nextState.row, nextState.col]))\n",
    "                if(currReward > maxReward):\n",
    "                    maxReward = currReward\n",
    "                    optimalAction = action\n",
    "            env.policy[currState.row, currState.col] = optimalAction\n",
    "                \n",
    "\n",
    "    print(\"Reward\\n\", env.reward)\n",
    "    print(\"Optimal Values\\n\",env.value)\n",
    "    print(\"Optimal Policy\\n\", env.policy)\n",
    "    \n",
    "    #Plot ||V_t - V_inf||_inf\n",
    "    optimalValue = env.value.copy()\n",
    "    env.value = np.zeros((env.rowNum, env.colNum))\n",
    "    iter1 = 0\n",
    "    diff = []\n",
    "    while(True):\n",
    "        if(iter1>1000):\n",
    "            break\n",
    "        iter1+=1\n",
    "\n",
    "        delta = 0\n",
    "        for row in range(env.rowNum):\n",
    "            for col in range(env.colNum):\n",
    "                currState = State(row, col)\n",
    "                value = -1\n",
    "                prevValue = env.value[row, col]\n",
    "                for action in env.action:\n",
    "                    currReward = 0\n",
    "                    for i, p in enumerate(env.probability[actionToIndex[action]]):\n",
    "                        succAction = indexToAction[i]\n",
    "                        nextState = env.getNextState(currState, succAction)\n",
    "                        if(nextState is None):\n",
    "                            currReward+=(p*-1)\n",
    "                        else:\n",
    "                            currReward+=(p*(env.reward[currState.row, currState.col]\n",
    "                                            + gamma* env.value[nextState.row, nextState.col]))\n",
    "\n",
    "                    value = max(value, currReward)\n",
    "\n",
    "                env.value[row, col] = value\n",
    "                delta = max(delta, abs(prevValue - env.value[row, col]))\n",
    "        if(delta < theta):\n",
    "            break\n",
    "        diff.append(normInfinity(env.value, optimalValue))\n",
    "    print(\"Plot of infinity norm of V_t - V*\")\n",
    "    plt.plot(diff, 'r-', label = \"V-V*\")\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.ylabel(\"Infinite norm\")\n",
    "    plt.title(\"|| V_t - V* ||_inf  vs  iteration\\n\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    print(\"_______________Value Iteration Completed_________________\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPolicy(env, policy, gamma):\n",
    "    value = np.zeros((env.rowNum, env.colNum))\n",
    "    \n",
    "    iter1 = 0\n",
    "    theta = 0.01\n",
    "    while(True):\n",
    "        if(iter1 > 1000):\n",
    "            break\n",
    "        iter1+=1\n",
    "        delta = 0\n",
    "        for row in range(env.rowNum):\n",
    "            for col in range(env.colNum):\n",
    "                prevVal = value[row, col]\n",
    "                currState = State(row, col)\n",
    "                action = policy[row, col]\n",
    "                \n",
    "                currReward = 0\n",
    "                for i, p in enumerate(env.probability[actionToIndex[action]]):\n",
    "                    succAction = indexToAction[i]\n",
    "                    nextState = env.getNextState(currState, succAction)\n",
    "                    if(nextState is None):\n",
    "                        currReward+=(p*-1)\n",
    "                    else:\n",
    "                        currReward+=(p*(env.reward[currState.row, currState.col]\n",
    "                                         + gamma* value[nextState.row, nextState.col])) \n",
    "                    \n",
    "                value[row, col] = currReward\n",
    "                delta = max(delta, abs(value[row, col] - prevVal))\n",
    "        if(delta < theta):\n",
    "            break\n",
    "    \n",
    "    policy = np.array([[\"right\", \"right\", \"right\", \"right\"]\n",
    "                        ,[\"right\", \"right\", \"right\", \"right\"]\n",
    "                        ,[\"right\", \"right\", \"right\", \"right\"]\n",
    "                        ,[\"right\", \"right\", \"right\", \"right\"]])\n",
    "    \n",
    "    for row in range(env.rowNum):\n",
    "        for col in range(env.colNum):\n",
    "            currState = State(row, col)\n",
    "            optimalAction = \"left\"\n",
    "            maxReward = -100\n",
    "            for action in env.action:\n",
    "                currReward = 0\n",
    "                for i, p in enumerate(env.probability[actionToIndex[action]]):\n",
    "                    succAction = indexToAction[i]\n",
    "                    nextState = env.getNextState(currState, succAction)\n",
    "                    if(nextState is None):\n",
    "                        currReward+=(p*-1)\n",
    "                    else:\n",
    "                        currReward+=p*(env.reward[currState.row, currState.col] + \n",
    "                                       gamma*value[nextState.row, nextState.col])\n",
    "                if(currReward > maxReward):\n",
    "                    maxReward = currReward\n",
    "                    optimalAction = action\n",
    "            \n",
    "            policy[row, col] = optimalAction\n",
    "            \n",
    "    return policy\n",
    "\n",
    "def getValue(env, policy, gamma):\n",
    "    value = np.zeros((env.rowNum, env.colNum))\n",
    "    iter1 = 0\n",
    "    theta = 0.01\n",
    "    while(True):\n",
    "        if(iter1 > 1000):\n",
    "            break\n",
    "        iter1+=1\n",
    "        for row in range(env.rowNum):\n",
    "            for col in range(env.colNum):\n",
    "                currState = State(row, col)\n",
    "                action = policy[row, col]\n",
    "                \n",
    "                currReward = 0\n",
    "                for i, p in enumerate(env.probability[actionToIndex[action]]):\n",
    "                    succAction = indexToAction[i]\n",
    "                    nextState = env.getNextState(currState, succAction)\n",
    "                    if(nextState is None):\n",
    "                        currReward+=(p*-1)\n",
    "                    else:\n",
    "                        currReward+=(p*(env.reward[currState.row, currState.col]\n",
    "                                         + gamma* value[nextState.row, nextState.col])) \n",
    "                    \n",
    "                value[row, col] = currReward\n",
    "    return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policyIteration(gamma):\n",
    "    print(\"\\n___________________Policy Iteration_____________________\")\n",
    "    env = Environment(4, 4)\n",
    "    policy = np.array([[\"right\", \"right\", \"right\", \"right\"]\n",
    "                        ,[\"right\", \"right\", \"right\", \"right\"]\n",
    "                        ,[\"right\", \"right\", \"right\", \"right\"]\n",
    "                        ,[\"right\", \"right\", \"right\", \"right\"]])\n",
    "    \n",
    "    for iter1 in (range(1000)):\n",
    "\n",
    "        prevPolicy = policy.copy()\n",
    "        policy = getPolicy(env, policy, gamma)\n",
    "        if(np.array_equal(policy, prevPolicy)):\n",
    "            print(\"Found optimal Policy\")\n",
    "            break\n",
    "        \n",
    "    print(\"Optimal Policy\\n\", policy)\n",
    "    print(\"Optimal Value\\n\", getValue(env, policy, gamma))\n",
    "    print(\"_________________Policy Iteration Completed________________\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Call Functions here "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'valueIteration' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-51407c0d99fe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvalueIteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgamma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.98\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mpolicyIteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgamma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.98\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'valueIteration' is not defined"
     ]
    }
   ],
   "source": [
    "valueIteration(gamma = 0.98)\n",
    "policyIteration(gamma = 0.98)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
